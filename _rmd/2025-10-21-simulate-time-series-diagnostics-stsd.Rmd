---
title: "Simulate Time Series Diagnostics with {sTSD}"
output:
  md_document:
    variant: gfm
    preserve_yaml: TRUE
author: "steve"
date: '2025-10-21'
excerpt: "Moses returned from on high with 10 commandments, though I'm not sure a similar routine is necessary for critical values from non-standard distributions in this day and age."
layout: post
categories:
  - Teaching
  - R
image: "mel-brooks-old-testament.jpg"
active: blog
---

```{r setup, include=FALSE, cache=F}

rmd_name <- knitr::current_input()
rmd_name <- stringr::str_sub(rmd_name, 12, -1)
rmd_name <- stringr::str_sub(rmd_name, 1, stringr::str_length(rmd_name)-4)


base_dir <- "~/Koofr/svmiller.github.io/"
base_url <- "/"
fig_path <- paste0("images/", rmd_name, "/")

cache_path <- paste0("~/Koofr/svmiller.github.io/cache/", rmd_name, "/")

add_jekyll_image <- function(url, caption, width, align) {
 img <- paste0('{% include image.html url="',url,'" caption="',caption,'" width=',width,' align="',align,'" %}')
 cat(img)
}

add_update <- function(announce, text) {
  
  update <- paste0('{% include updatebox.html announce="',announce,'" text="',text,'" %}')
 cat(update)
  
}

add_announce <- function(announce, text) {
  
  update <- paste0('{% include announcebox.html announce="',announce,'" text="',text,'" %}')
 cat(update)
  
}

knitr::opts_knit$set(base.dir = base_dir, base.url = base_url)
knitr::opts_chunk$set(fig.path = fig_path, dpi= 300,
                      cache.path = cache_path,
                      fig.width = 11,
                      message=FALSE, warning=FALSE,
                      cache = FALSE,
                      collapse = TRUE, comment = "#>") 

# library(tidyverse)     # for most things
# library(stevemisc)     # for graph formatting
# library(kableExtra)    # for tables
# library(stevedata)
# library(modelsummary)
# library(stevethemes)
# library(modelr)
# library(lmtest)
# library(sandwich)
# library(ggdist)
# library(ggrepel)
library(tidyverse)
library(stevedata)
library(stevethemes)
library(sTSD)
library(ggh4x)

options(knitr.kable.NA = '')

# theme_set(theme_steve())
```


```{r leadimage, echo=F, eval=T, results="asis", cache=F}
add_jekyll_image('/images/mel-brooks-old-testament-crop.jpg', "You don't have to treat critical values from non-standard distributions as gospel in this day and age. See them for yourself.", "400", "right")
```

<!-- *Last updated: `r format(Sys.Date(), "%d %B %Y")`.*  -->

I'm writing this for an advanced quantitative methods class I teach in our Master's program in the Department of Economic History and International Relations. A department of two somewhat disparate disciplines, the curriculum we have at the Master's level endeavors to teach students interested in both basic disciplines at a foundation level. That concerns the quantitative methods sequences (in which I feature prominently) and concerns time series topics (which feature more in quantitative economic history and not international relations as much). Thus, moving here meant I needed to teach myself some things I had never needed to learn before as either part of my methods training or as a tool for some project I was doing.

Enter the wonderful world of unit root testing, a common procedure for time series analyses where a data does not hover around some constant mean. If this describes your data---and it sure as hell describes just about any series that you might be watching on [Yahoo Finance](https://finance.yahoo.com/)---then a failure to adequately diagnose it could lead you down a path to a slew of errors. You have multiple options for how you might diagnose so-called "non-stationarity" or "unit roots",[^int] but the classic one is the Dickey-Fuller test. There is no shortage of packages available for doing unit root tests in R. The familiar [`{tseries}`](https://cran.r-project.org/web/packages/tseries/index.html) will do it, as will [`{urca}`](https://cran.r-project.org/web/packages/urca/index.html) and [`{aTSA}`](https://cran.r-project.org/web/packages/aTSA/index.html) (among many others, I'm sure). All work for what they're intended to do, but all will make evident one of the more whimsical/frustrating things about this procedure. It's a test that produces a *p*-value, for which there is a null hypothesis, but it references a non-standard distribution whose critical values themselves were once simulated over 50 years ago on a Canadian super computer of its time. The end result leaves the baffled student and the professor teaching himself this procedure imagining themselves in the crowd while Moses returns with [the 10 (or 15) commandments](https://www.youtube.com/watch?v=-8ihcq4hzR4). "Interesting. Makes sense? But you're leaving me wanting to see this for myself."

[^int]: One frustrating thing jumping into this topic has been the slew of synonyms to describe the same basic thing. A time series that looks like a familiar asset price (e.g. [the ARCA Steel Index](https://finance.yahoo.com/quote/%5ESTEEL/)), might be "non-stationary", have a "unit root", or could even be "integrated". The last of these is often the most frustrating thing to encounter. A stationary time series in level form is also "integrated", if at order 0. [It would look something like a problem-free trace plot](http://amosdevelopment.com/webhelp/imp-post-radiotrace.html) from a Markov Chain Monte Carlo (MCMC) procedure. A time series that is integrated at order 1 would be a random walk, like the kind I'll explore in this post. A first difference makes such a time series to be stationary. A time series integrated at order 2 would be an "explosive" time series, like the kind you might see of the Dow Jones Industrial Average over its entire lifespan (i.e. since 1885). Its growth in level terms looks exponential, should probably be log-transformed to avoid this (unless you're dealing with, God help you, [Bitcoin](https://finance.yahoo.com/quote/BTC-USD/)), and can be "double-differenced" to be stationary. I'm unaware of integration at levels beyond that, but there's still a lot I don't know about time series topics. 

When you understand what it's doing, it really takes no effort to do this yourself. The R package I wrote to do this, [`{sTSD}`](https://svmiller.com/sTSD/), handles the (Augmented) Dickey-Fuller test and related [Phillips-Perron](https://doi.org/10.1093%2Fbiomet%2F75.2.335) and [Kwiatkowski et al. (KPSS)](https://doi.org/10.1016/0304-4076(92)90104-Y) procedures in the same way.[^null] It takes an assumed time series you give it and simulates some user-specified number of time series that matches it description, either stationary or non-stationary.

[^null]: There is a small caveat about the null hypothesis for the KPSS test, which by default is "stationarity". The null hypothesis in the (Augmented) Dickey-Fuller and Phillips-Perron tests is "non-stationarity." The latter two tests are anomalous from my vantage point of diagnostic tests because the null hypothesis is "you have a problem." Compare to other diagnostic procedures we typically teach, like a Breusch-Pagan test, Durbin-Watson test, or Breusch-Godfrey test. Among unit root tests of which I'm aware, KPSS is unique for its null hypothesis. This affects the default behavior of the related functions in the `{sTSD}` package, but it does not affect what you could materially do with the simulations in this package.

Here are the R packages that will appear in this post.

```r
library(tidyverse)   # for most things...
library(stevethemes) # for my themes/themeing elements
library(sTSD)        # the star of the show
library(ggh4x)       # for a more flexible nested plot element in {ggplot2}
```

Here's a table of contents.

1. [What Are These Unit Root Tests Trying to Accomplish?](#intuition)
2. [About That t-Statistic, Though...](#about)
3. [What {sTSD} Does](#stsd)
4. [See It For Yourself, with an Applied Example](#example)
5. [Conclusion](#conclusion)


## What Are These Unit Root Tests Trying to Accomplish? {#intution}

I get the basis of this question from watching [an interview with Bradley Efron in 2014](https://purl.stanford.edu/hj183nm6372), where he discusses the origin of the bootstrap as a question about what its predecessor (the jackknife) was trying to accomplish. Every statistical procedure is trying to accomplish something with some question or problem in mind. The Dickey-Fuller test, its augmented form, and its successors (Phillips-Perron, KPSS) are no different.

We know that non-stationarity poses considerable problems for statistical inference and joint non-stationarity may even lead to problems of [spurious regression](http://www.fsb.miamioh.edu/lij14/672_2014_s8.pdf). What procedure might warn us of these potential issues? Consider the case of the simple random walk, where the observation of *y* at time point *t* is a function of its previous value and some random shock/error. We'd express this form as follows.

$$
y_{t} = \rho y_{t-1} + e_{t}
$$

This coefficient ($$\rho$$) communicates the kind of memory in the series and has a bound between 0 and 1. If $$\rho$$ is 0, there is no memory whatsoever in the series and the series itself becomes basic noise over time. If $$\rho$$ is 1, there is lasting memory in the series. Sometimes likened to a "drunkard's walk", the series has a critical path dependence like a drunkard stumbling out of the bar and toward nowhere in particular. There is randomness, but each "step" along the way is maximally dependent on the steps (and previous randomness) before it.

Conceptualizing a way out of this doesn't take too much effort. It actually takes a simple pen and the nearest available piece of paper or napkin. Start by subtracting the previous observation ($$y_{t-1}$$) from the current observation ($$y_t$$) and you get the familiar difference term ($$\Delta y_t$$). If you understand that the previous observation ($$y_{t-1}$$) is unweighted with a value of 1, you get this derivation and the introduction of a new coefficient ($$\gamma$$) to summarize this relationship. 

$$
\begin{aligned} 

y_{t} &= \rho y_{t-1} + e_{t} \\
y_{t} - y_{t-1} &= \rho y_{t-1} - (1)(y_{t-1}) + e_{t} \\
y_{t} - y_{t-1} &= (\rho - 1) y_{t-1} + e_{t} \\
\Delta y_t &= \gamma y_{t-1} + e_{t}
\end{aligned}
$$

The formula syntax may not be welcome for beginners, but the principle is pretty straightforward. A maximally autoregressive series ($$\rho$$ = 1) is, we would say, first-difference stationary on paper. When this is true, $$\gamma$$ (or $$\rho - 1$$) is 0 and the previous value should tell you absolutely nothing about the change in the current value from the previous value. Only $$e_t$$ remains to account for the difference in a series with total information retention.

Simulation by way of fake data shows how this works. After setting a reproducible seed, we'll create a simple series of 500 time units. Then, we'll generate random noise (`e`) and allow the outcome (`y`) to be the cumulative sum of this random noise. Whereas the cumulative sum maximally weights the previous observation, and by extension all those before it, this is the pure random walk. The first difference of this (`d_y`) returns us just the noise at the expense of the first observation in the series.

```{r}
set.seed(8675309) # Jenny, I got your number...
tibble(t = 1:500, # 500 time units
       e = rnorm(500), # 500 random numbers
       y = cumsum(e),  # the random walk, then...
       # the first difference.
       d_y = y - lag(y)) -> fakeWalk

fakeWalk # let's see the data
```

Here's what a plot of these data would look like.

```{r random-walk-first-difference, echo = F, eval = T}
# Let's plot the data...
fakeWalk %>%
  gather(var, val, -t, -e) %>%
  mutate(cat = fct_inorder(ifelse(var == "y", "Value in Series (Random Walk)", "First Difference"))) %>%
  ggplot(.,aes(t, val)) +
  theme_steve() +
  geom_line() + 
  facet_wrap(~cat, scales='free_y') +
  labs(x = "Time", y = "Value",
       title = "A Random Walk is Non-Stationary; Its First Difference is Stationary",
       subtitle = "There's nothing fancy happening here that you couldn't discern with pen and paper.")
```


What happens when we lose this kind of "memory" in the series? In any instance where $$\rho$$ < 1, $$\gamma$$ in the above equation is necessarily negative. Thus, the past observation starts to tells you something about the next period's change through something analogous to a correction mechanism or mean reversion. Past values above the mean in $$y$$ get "corrected" and pull back to the mean of the overall series. Past values below the mean in $$y$$ get corrected and pull up to the mean of the overall series. Again, referenced to the above formula, $$\gamma$$ is negative. Referenced to some kind of linear model of this mechanism, the *t*-statistic you get predicting the current period's change with the last observed value will come back *more* negative than it would in the pure random walk. There's a correction, and we can feel more "confident" predicting such a correction in the presence of even partial memory. Observe in the case where $$\gamma$$ is -.5 (i.e. $$\rho$$ in `arima.sim()` is .5).


```{r}
tibble(t = 1:500,
       y = as.vector(arima.sim(n = 500, list(ar = 0.5), sd = 1)), 
       d_y = y - lag(y)) -> fakeAR

# Compare the pure random walk...
summary(M1 <- lm(d_y ~ 0 + lag(y), fakeWalk))$coefficients

# ...with one that has partial memory.
summary(M2 <- lm(d_y ~ 0 + lag(y), fakeAR))$coefficients
```

The past value tells you nothing of the current value's change in the random walk, but it tells you a lot in the case where memory is just partial and the series corrects to its mean. That predictive power is communicated in the *t*-statistic of a simple linear model.

## About That t-Statistic, Though... {#about}

It would be ideal if that that *t*-statistic of the model were sufficient for making inferential claims, but it isn't. That is indeed the test statistic of note, but the statistic doesn't follow the inferential process of the simple linear model because the test is over the residual term and not the raw data of the series. Instead, we need some other set of critical values for making inferential claims about non-stationarity in the time series. Enter the following table, which first appeared in either Fuller (1976) or Dickey (1976). I have Dickey's (1976) dissertation, if not Fuller's (1976) textbook, so I'll present the Dickey (1976) version.

```{r dickeytab, echo=F, eval=T, results="asis", cache=F}
add_jekyll_image('/images/dickey1976tab53-full.png', "Critical Values of the Dickey-Fuller Test, by Way of Dickey (1976)", "659", "center")
```

For added context, these values are the product of an extensive Monte Carlo simulation carried out with assistance from a supercomputer at McGill University at the time ("Super Duper"). Assume a standard normal distribution (with a mean of 0 and a standard deviation of 1). Now, simulate (in the table's case) a data set of length 25, 50, 100, 250, 500, 750, and $$\infty$$.[^inf] Further assume the pure random walk from the above and related data-generating processes for which there is a *y*-intercept (i.e. a "drift", in time series parlance) and a linear time trend. For each of those data sets corresponding to each of those data-generating processes, replicate these data sets anywhere from 10,000 to 100,000 times. Get the *t*-statistics for each of those replicates as a kind of distribution of test statistics under the assumption of a non-stationary time series. Break those into percentiles corresponding with anywhere from 1% to the 99%. Recall that the test statistics are intended to come back negative for real-world cases, so we should focus our attention on those magic numbers like .05 or .10 corresponding with the left tail. 

This is where one of the frustrating components of this procedure comes: the null hypothesis. This is one of those type of procedures. The null hypothesis here is clumsily stated as equivalent to "problem", in contrast with other diagnostic tests like Breusch-Pagan, Breusch-Godfrey, and Durbin-Watson (in which the null hypothesis is "no problem"). If the test statistic is less than ("more negative than") one of those critical values of your choice, you can reject the null hypothesis of "problem" (i.e. non-stationarity) and instead accept the alternative hypothesis of "no problem" (i.e. stationarity). If you cannot reject the null hypothesis, you have a problem.

This is a case where the logic is nifty but the execution has left me a bit wanting ever since I had to start teaching about this procedure. For one, literally every textbook reproduces this exact table from either Fuller's (1976) textbook or Dickey's (1976) dissertation. I don't doubt the output of the model, but its utility is defined by implicit assumptions in a 50-year-old supercomputer and the multiple other assumptions Dickey and Fuller built into the procedure. Two, much like anything involving a *t*-statistic, it is never offered in relation to your actual data. I will never have a data set of 25, 50, 100, 250, 500, 750, or $$\infty$$ observations, so what these statistics mean for my time series of 336 observations or 83 observations has to be approximated or interpolated through some other means. This makes it kind of biblical, in a way. It's again similar to Moses coming down on high with ten commandments ad infinitum without any real means to square ten simple dictates with the exigencies of real life. The third complaint isn't really the fault of anyone in particular, but this function is non-standard. Summary by simulation was the only way to go and Dickey and Fuller do at least provide an honest framework based on simulation that very few people could do 50 years ago. But I can do this now.[^seoul] All of us have better technology than Dickey and Fuller had 50 years ago. Why not? It would certainly circumvent some of the awkwardness of doing inference by the null hypothesis. Plus, I can simulate based on features about my time series rather than the standard normal distribution.

[^seoul]: In fact, I wrote this particular passage on my laptop somewhat antsy for things to do on a 14-hour flight from Seoul to Amsterdam.

[^inf]: There does not appear to be a numeric plug-in for infinity (e.g. 1,000 or 10,000). Dickey (1976, 49-50) seems to be describing a limit function in which these statistics are analytically derived.

## What {sTSD} Does {#stsd}

`{sTSD}` is born from my frustration with these procedures, and also my affinity for simulating things. [`sadf_test()`](https://svmiller.com/sTSD/reference/sadf_test.html), which handles both the Dickey-Fuller and its "augmented" corollary, looks like this. 

```r
sadf_test(x, n_lags = NULL, n_sims = 1000, sim_hyp = "nonstationary")
```

`sadf_test()` takes a vector of an assumed time series (`x`). It then asks for some number of simulations (`n_sims`) you would like to do, with a default of 1,000. Thereafter, it will simulate the user-specified number of simulations from either a pure white noise time series (`sim_hyp = 'stationary'`) or three different time series (`sim_hyp = 'nonstationary'`) where the data are either a pure random walk, a random walk with a drift (y-intercept), or a random walk with a drift and trend (i.e. y-intercept and time trend). It then runs the Dickey-Fuller or its "augmented" version on all those simulated series, contingent on what you provide to the `n_lags` argument in the function.[^lags] It will allow you to assess whether your time series is stationary or non-stationary by comparison to simulated series of the exact length of your series that is known to be stationary or non-stationary in some form.

[^lags]: The choice of lagged first differences is what makes the Augmented Dickey-Fuller test to be "augmented." It's also not something a lot of econometrics textbooks I've seen belabor in any detail. [`adf_lag_select()`](https://svmiller.com/sTSD/reference/adf_lag_select.html) might be of interest to you if you want to consider some thresholds for optimal lag selection tailored for your series while [`lag_suggests`](https://svmiller.com/sTSD/reference/lag_suggests.html) is a data set that will straight-up tell you what are some suggested first differences to specify, based on past scholarship. Do with those what you will, but it's one reason why I would prefer to teach unit root tests around either the Phillips-Perron or KPSS procedures. In both cases, you ask for some kind of long- or short-term lag for the bandwidth/kernel generating the test statistic. If standard texts don't belabor the lag selection procedure, doing an alternative test that doesn't ask that information of you seems to make more sense.

Let's do a quick Dickey-Fuller test (`n_lags = 0`) with just 100 simulations to make this quick, and to explore it's basic output. The output will come back as a list with a specialty class provided by the function.

```{r}
DF1 <- sadf_test(fakeWalk$y, n_lags = 0, n_sims = 100)
class(DF1)
names(DF1)
```

The first element, (`"stats"`), is the test statistics. In order, they are the test statistic for the Dickey-Fuller test with 1) no drift nor trend, 2) drift, no trend, and 3) drift and trend. You can compare with it communicates with the corollary functions in the `{urca}` and `{aTSA}` package.

```{r}
DF1$stats

# compare with in {urca}:
attributes(urca:::ur.df(fakeWalk$y, type = "none", lags = 0))$teststat[1]
attributes(urca:::ur.df(fakeWalk$y, type = "drift", lags = 0))$teststat[1]
attributes(urca:::ur.df(fakeWalk$y, type = "trend", lags = 0))$teststat[1]

# {aTSA} does all three in one fell swoop, but be mindful it assumes lag of 1 is
# a lag of 0. There is some processing issues it does underneath the hood that
# account for this.
aTSA::adf.test(fakeWalk$y, nlag = 1)
```

The last element (`"attributes"`) contains information for post-processing in another function I will introduce later, but let's take a look at the second element (`"sims"`). This is a data frame that is always equal to three times the number of simulations you requested. In our case, these would be the first nine of those simulations.

```{r}
head(DF1$sims, 9)
```

Recall that we leaned on the default procedure for a Dickey-Fuller test, which is to assume non-stationarity of some particular form: the pure random walk, the random walk with a drift, and the random walk with a drift and deterministic time trend. For each simulation, we generated a known series of the length of our time series that matches that description we are testing.[^sim_df_mod] Each simulation then has three randomly generated series for which it calculates Dickey-Fuller test statistics. Those statistics are stored here and can be summarized, visually, however you want.

[^sim_df_mod]: [`sim_df_mod()`](https://svmiller.com/sTSD/reference/sim_df_mod.html) is mostly intended for internal use as a helper function, but it's generating these different time series. In particular, it leans on the [Rademacher distribution](https://en.wikipedia.org/wiki/Rademacher_distribution) to generate drift and trend effects. I am unaware of many texts belaboring these details when they do simulate them, but the texts I have found could be reasonably approximated with this distribution.

Perhaps the easiest thing is to lean on the `ur_summary()` function for you based on the information included in all elements of the object `sadf_test()` returns. `"stats"` has the test statistics, `"sims"` has the raw simulations, and `"attributes"` has a quick summary of the arguments fed to the `sadf_test()` function. Applied to our test, we get the following.

```{r}
ur_summary(DF1)
```

In our case, the simulated time series to which we are comparing our time series is non-stationary. Knowing what we know from the pen-and-napkin math above, we expect a non-stationary time series to approximate a *t*-distribution whose central tendency hovers on 0 (even though we can't call it a *t*-distribution). When information retention is partial (i.e. $$\rho$$ < 1), the coefficient predicting first differences becomes "more" negative and can be better discerned from 0 in the pen-and-napkin math above. Thus, you judge the test statistic by how negative it is and how easily it could be discerned from a distribution of test statistics generated from a random walk with permanent information retention. In our case, we have an obvious random walk. Its test statistic is very much compatible with a distribution of test statistics ($$\tau$$) that could be generated from a random walk. We cannot reject the null hypothesis of a non-stationary times series because, well, we generated a random walk. Duh.

Compare the above with a pure white noise times series and the one with partial memory. Even the one with partial memory has "shocks" today that decay geometrically. The series "forgets" past shocks pretty quickly, all things considered. The series with absolutely no information retention (i.e. the one generated by `rnorm()`) can more confidently reject 0.

```{r}
# partial memory
DF2 <- sadf_test(fakeAR$y, n_lags = 0, n_sims = 100)
ur_summary(DF2)

# pure white noise
DF3 <- sadf_test(rnorm(500), n_lags = 0, n_sims = 100)
ur_summary(DF3)
```

## See It For Yourself, with an Applied Example {#example}

You can better see this for yourself with actual data. `USDSEK` is a time series included in `{sTSD}` that has information I find good to know since I moved to Sweden from the United States: the Swedish crown (SEK) and U.S. dollar (USD) exchange rate. In particular, how many Swedish crowns are necessary to obtain one dollar? This is an interesting time series that you can see for yourself here.

```{r usd-sek-time-series, echo = F, eval = T, fig.width = 12}
USDSEK %>% 
  ggplot(., aes(date, close)) +
  theme_steve() +
  geom_line() + 
  scale_y_continuous(labels = scales::dollar_format(prefix = "SEK ")) +
  labs(title = "The SEK/USD Exchange Rate, 2010-2024",
       y = "Swedish Crowns Necessary to Obtain One Dollar at Daily Close",
       x = "", caption = "Data: ?USDSEK in {sTSD}",
       subtitle = "Ideally, the exchange rate hovers on 7/1, but events prior to 2025 led to an appreciation of the dollar over smaller currencies like the Swedish crown.")
```

This sure looks like it would have a strong, built-in information retention mechanism. Most time series of commodities that are traded daily have a pervasive, built-in memory. We can see for ourselves with the `sadf_test()` function and lean on it to identify an appropriate lag structure for us.

```{r}
DF4 <- sadf_test(USDSEK$close, n_sims = 500)
DF5 <- sadf_test(diff(USDSEK$close), sim_hyp = "stationary", n_sims = 500) # I'm going somewhere with this...

ur_summary(DF4)
```

The test statistics are compatible with a distribution of test statistics of a random walk, leading us to reasonably conclude that our times series of a currency exchange rate traded daily is non-stationary. Of course it would be. 

However, it would be illustrative to get an idea of what this looks like, visually. Here, the tests included in `{sTSD}` allow you to evaluate your time series against a stationary or non-stationary time series. Inference in the case of the Dickey-Fuller test is traditionally made against a non-stationary time series. However, you could simulate against a stationary time series to get an idea of what the test statistics would resemble for a time series with the length and number of lags requested. That, I think, is one feature missing when your critical values are handed down from on high based on what was computationally possible or feasible 50 years ago. We know from pen-and-napkin that the first difference should be stationary, so let's also show what the test statistics from the first-difference time series looks like compared to a distribution of test statistics from a stationary time series.

```{r, echo = F, eval = T, fig.width = 12}
DF4$sims %>% mutate(hyp = "Non-stationary (Actual SED/USK Time Series)") %>%
  bind_rows(., DF5$sims %>% 
              mutate(hyp = "Stationary (First Difference of SED/USK Time Series")) -> DFC
  
# Trust me, I'm a doctor...
DFC %>%
  distinct(hyp, cat) %>%
  mutate(teststat = c(as.vector(DF4$stats), 
                      as.vector(DF5$stats))) -> teststats
```

```{r simulated-test-stats-usd-sek, echo = F, eval = T, fig.width = 12}
DF4$sims %>% mutate(hyp = "Non-stationary (Actual SED/USK Time Series)") %>%
  bind_rows(., DF5$sims %>% 
              mutate(hyp = "Stationary (First Difference of SED/USK Time Series)")) -> DFC
  
# Trust me, I'm a doctor...
DFC %>%
  distinct(hyp, cat) %>%
  mutate(teststat = c(as.vector(DF4$stats), 
                      as.vector(DF5$stats))) -> teststats

DFC %>%
  mutate(cat = fct_inorder(cat)) %>%
  ggplot(., aes(tau)) +
  theme_steve() +
  geom_density() +
  facet_nested_wrap(~hyp + cat, nrow = 2, ncol=3) + 
  geom_vline(data = teststats, aes(xintercept = teststat),
             linetype = "dashed", color = g_c("su_fire"), 
             linewidth = 1.1) +
  labs(x = "Distribution of Test Statistics",
       y = "",
       title = "A Visual Assessment of Our Test Statistics Against Simulated Stationary and Non-Stationary Time Series",
       subtitle = "The test statistics of the actual time series are compatible with a simulated distribution of a random walk.\nThe test statistics of the first differences are compatible with a simulated distribution of white noise.")
```

## Conclusion {#conclusion}

There really isn't much to conclude here. It's more of a quick introduction/tutorial for the MA students in our department who have to learn about unit root testing with [my `{sTSD}` package](https://svmiller.com/sTSD/). I never quite liked the R packages that were available in how they communicated the information of interest. Plus, it doesn't make much sense these days to rely on old critical values generated 50 years ago for non-standard distributions like the one underpinning the (Augmented) Dickey-Fuller test. You can simulate those for yourselves. Moses can give you commandments and you just run with premise, supposedly. You can do the same here if you'd like, but simulation is much more informative.